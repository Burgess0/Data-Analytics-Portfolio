{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从数据爬取到简单词云的完整流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来自[用python爬取微博数据并生成词云](https://www.cnblogs.com/yangshunde/p/7742868.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码，哪里报错，安装什么包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.分析网址"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开微博移动端网址 https://m.weibo.cn/searchs ，找到待分析用户的微博ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"m_weibo_search.jpg\", width=620, heigth=440>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"njtech.jpg\", width=320, heigth=240>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 进入微博主页，chrome“开发者工具”，分析浏览器发送请求的过程\n",
    "+ 打开开发者工具后，需要刷新左侧网页，收集数据\n",
    "+ 打开 Chrome 浏览器的调试功能，选择 Network 菜单，观察到获取微博数据的的接口是 https://m.weibo.cn/api/container/getIndex ，后面附带了一连串的参数，这里面有些参数是根据用户变化的，有些是固定的，先提取出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"get_request.jpg\", width=820, heigth=540>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ cardlistInfo: {containerid: \"1076031880207035\", v_p: 42, show_style: 1, total: 11173, since_id: 4301251451406395}\n",
    "+ containerid: \"1076031880207035\"\n",
    "+ show_style: 1\n",
    "+ since_id: 4301251451406395\n",
    "+ total: 11173\n",
    "+ v_p: 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 再来分析接口的返回结果，返回数据是一个JSON字典结构，total 是微博总条数，每一条具体的微博内容封装在 cards 数组中，具体内容字段是里面的 text 字段。很多干扰信息已隐去。\n",
    "+ 可以把Preview-cards-0、1、2、...10展开对照左侧数据，就能看到具体的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.构建请求头和查询参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析完网页后，我们开始用 requests 模拟浏览器构造爬虫获取数据，因为这里获取用户的数据无需登录微博，所以我们不需要构造 cookie信息，只需要基本的请求头即可，具体需要哪些头信息也可以从浏览器中获取，首先构造必须要的请求参数，包括请求头和查询参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"id_information.jpg\", width=520, heigth=440>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ uid: 1880207035\n",
    "+ luicode: 10000011\n",
    "+ lfid: 100103type=1&q=南京工业大学\n",
    "+ type: uid\n",
    "+ value: 1880207035\n",
    "+ containerid: 1076031880207035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构造简单爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过返回的数据能查询到总微博条数 total，爬取数据直接利用 requests 提供的方法把 json 数据转换成 Python 字典对象，从中提取出所有的 text 字段的值并放到 blogs 列表中，提取文本之前进行简单过滤，去掉无用信息。顺便把数据写入文件，方便下次转换时不再重复爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import codecs # python编码模块\n",
    "import re # python正则表达式模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse #分词是必须的，分词分析\n",
    "import matplotlib.pyplot as plt #想画个词云，matplotlib是必须的\n",
    "import requests #爬虫请求包\n",
    "from scipy.misc import imread #scipy数据分析包\n",
    "from wordcloud import WordCloud # 词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'liuzhijun' #版权申明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带爬取页面的头信息\n",
    "headers = {\n",
    "    \"Host\": \"m.weibo.cn\",\n",
    "    \"Referer\": \"https://m.weibo.cn/u/1880207035?uid=1880207035&t=0&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%8D%97%E4%BA%AC%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 定义一个函数clean_html，这个函数是把text中我们想分析的对象整体删除\n",
    "+ [re.compile](https://www.cnblogs.com/nomorewzx/p/4203829.html)参考这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数是删除文本中的杂乱信息，只保留文本 ##采用正则表达式\n",
    "def clean_html(raw_html):\n",
    "    pattern = re.compile(r'<.*?>|转发微博|//:|Repost|，|？|。|、|分享图片|回复@.*?:|//@.*')\n",
    "    text = re.sub(pattern, '', raw_html)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交待爬虫函数fetch_data需要的参数\n",
    "url = \"https://m.weibo.cn/api/container/getIndex\"\n",
    "# uid和containerid在运行主函数中赋值\n",
    "params = {\"uid\": \"{uid}\",\n",
    "          \"luicode\": \"10000011\",\n",
    "          #\"featurecode\": \"20000320\",\n",
    "          \"type\": \"uid\",\n",
    "          \"value\": \"1880207035\",\n",
    "          \"containerid\": \"{containerid}\",\n",
    "          \"page\": \"{page}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(uid=None, container_id=None):\n",
    "    \"\"\"\n",
    "    抓取数据，并保存到txt文件中\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    page = 0\n",
    "    total = 60 ## 这个参数很重要，不然大V的微博会耗时很久试一试20条\n",
    "    blogs = []\n",
    "    for i in range(0, total // 10): ## //是整除的意思 # 就是爬取该号十分之一（页）的内容\n",
    "        params['uid'] = uid\n",
    "        params['page'] = str(page)\n",
    "        params['containerid'] = container_id\n",
    "        # requests包是爬虫的核心，它的三个参数，已经定义好\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        cards = res.json().get(\"data\").get(\"cards\")\n",
    "\n",
    "        for card in cards:\n",
    "            # 每条微博的正文内容\n",
    "            # 打开“开发者工具”看一看，Preview\n",
    "            if card.get(\"card_type\") == 9:\n",
    "                text = card.get(\"mblog\").get(\"text\")\n",
    "                text = clean_html(text)\n",
    "                blogs.append(text)\n",
    "        page += 1\n",
    "        print(\"抓取第{page}页，目前总共抓取了 {count} 条微博\".format(page=page, count=len(blogs)))\n",
    "        # weibo.txt是事先创建的\n",
    "        with codecs.open('weibo1.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(blogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 创建词云函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image():\n",
    "    data = []\n",
    "    # stopwords是网上下载的，一个融合的停用词表\n",
    "    jieba.analyse.set_stop_words(\"./stopwords.txt\")\n",
    "    \n",
    "    # 对爬取的txt文本进行处理\n",
    "    with codecs.open(\"weibo1.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        for text in f.readlines():\n",
    "            data.extend(jieba.analyse.extract_tags(text, topK=20)) #其中text为待提取的文本，topK为返回几个TF/IDF权重最大的关键词，默认值为20。\n",
    "        data = \" \".join(data)\n",
    "        # 词云背景图\n",
    "        #mask_img = imread('./njtech_logo_qiang.jpg', flatten=True)\n",
    "        # 多试几个背景图片，出图效果和背景图的大小、色系、像素相关\n",
    "        mask_img = imread('./njtech_logo1.jpg', flatten=True)\n",
    "        wordcloud = WordCloud(\n",
    "            font_path='msyh.ttc', ##微软雅黑字体\n",
    "            background_color='white',\n",
    "            mask=mask_img\n",
    "        ).generate(data)\n",
    "        \n",
    "       #  plt.title(\"Njtech Weibo WordCloud\")\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig('./njtech_weibo1.jpg', dpi=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f932d0faad96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 主函数（执行）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1768409523\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1005051768409523\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## 给爬虫函数输入两个重要参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mgenerate_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bd765d4f87b4>\u001b[0m in \u001b[0;36mfetch_data\u001b[1;34m(uid, container_id)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cards\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mcard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcards\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m# 每条微博的正文内容\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# 打开“开发者工具”看一看，Preview\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 主函数（执行）\n",
    "if __name__ == '__main__':\n",
    "    fetch_data(\"1880207035\", \"1005051880207035\") ## 给爬虫函数输入两个重要参数\n",
    "    generate_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
