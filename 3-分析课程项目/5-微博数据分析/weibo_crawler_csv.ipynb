{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import codecs # python编码模块\n",
    "import re # python正则表达式模块\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (D:\\App\\anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15944/4281139049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m \u001b[1;31m#想画个词云，matplotlib是必须的\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;31m#爬虫请求包\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m \u001b[1;31m#scipy数据分析包\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m \u001b[1;31m# 词云\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (D:\\App\\anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import jieba.analyse #分词是必须的，分词分析\n",
    "import matplotlib.pyplot as plt #想画个词云，matplotlib是必须的\n",
    "import requests #爬虫请求包\n",
    "from scipy.misc import imread #scipy数据分析包\n",
    "from wordcloud import WordCloud # 词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带爬取页面的头信息\n",
    "headers = {\n",
    "    \"Host\": \"m.weibo.cn\",\n",
    "    \"Referer\": \"https://m.weibo.cn/u/1768409523?uid=1768409523&t=0&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%AD%A6\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交待爬虫函数fetch_data需要的参数\n",
    "url = \"https://m.weibo.cn/api/container/getIndex\"\n",
    "# uid和containerid在运行主函数中赋值\n",
    "params = {\"uid\": \"{uid}\",\n",
    "          \"luicode\": \"10000011\",\n",
    "          #\"featurecode\": \"20000320\",\n",
    "          \"type\": \"uid\",\n",
    "          \"value\": \"1768409523\",\n",
    "          \"containerid\": \"{containerid}\",\n",
    "          \"page\": \"{page}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()+\"/weibo.csv\"\n",
    "#path = \"/weibo.csv\"\n",
    "# 注意这里的编码是utf-8-sig，如果是utf-8写入的文件是乱码\n",
    "csvfile = open(path,'a',encoding='utf-8-sig',newline='')\n",
    "#clearData(csvfile)\n",
    "#每一次重新运行程序的时候把上一次保存的数据清空 #是重新运行程序#省区手动清空的多余环节\n",
    "csvfile.seek(0,0)\n",
    "csvfile.truncate()\n",
    "writer = csv.writer(csvfile)\n",
    "#writer.writerow(('username','source','comment'))\n",
    "writer.writerow(('text','attitudes_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数是删除文本中的杂乱信息，只保留文本 ##采用正则表达式\n",
    "## 按图索骥，查看数据结构，就知道文本的样子\n",
    "def clean_html(raw_html):\n",
    "    pattern = re.compile(r'<.*?>|转发微博|//:|Repost|，|？|。|、|分享图片|回复@.*?:|//@.*')\n",
    "    text = re.sub(pattern, '', raw_html)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(uid=None, container_id=None):\n",
    "    \"\"\"\n",
    "    抓取数据，并保存到txt文件中\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    page = 0\n",
    "    total = 100 ## 这个参数很重要，不然大V的微博会耗时很久试一试20条\n",
    "    blogs = []\n",
    "    # 循环页\n",
    "    for i in range(0, total // 10): ## //是整除的意思 # 就是爬取该号十分之一（页）的内容\n",
    "        params['uid'] = uid\n",
    "        params['page'] = str(page)\n",
    "        params['containerid'] = container_id\n",
    "        # requests包是爬虫的核心，它的三个参数，已经定义好\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        cards = res.json().get(\"data\").get(\"cards\")\n",
    "        \n",
    "        #循环每页中的每条微博\n",
    "        for card in cards:\n",
    "            # 每条微博的正文内容\n",
    "            # 打开“开发者工具”看一看，Preview\n",
    "            if card.get(\"card_type\") == 9:\n",
    "                text = card.get(\"mblog\").get(\"text\")\n",
    "                text = clean_html(text)\n",
    "                blogs.append(text) #计算爬取微博的条数\n",
    "                attitudes_count = card.get(\"mblog\").get(\"attitudes_count\")\n",
    "                \n",
    "                ## 把以上获得的文本数据和点赞数量保存到csv文件中\n",
    "                ## 注意这里的逻辑，是怎么循环处理的\n",
    "                writer.writerow((text, attitudes_count))\n",
    "        page += 1\n",
    "        print(\"抓取第{page}页，目前总共抓取了 {count} 条微博\".format(page=page, count=len(blogs)))\n",
    "        # weibo.txt是事先创建的\n",
    "        #with codecs.open('weibo1.txt', 'w', encoding='utf-8') as f:\n",
    "        #    f.write(\"\\n\".join(blogs))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抓取第1页，目前总共抓取了 10 条微博\n",
      "抓取第2页，目前总共抓取了 20 条微博\n",
      "抓取第3页，目前总共抓取了 29 条微博\n",
      "抓取第4页，目前总共抓取了 39 条微博\n",
      "抓取第5页，目前总共抓取了 49 条微博\n",
      "抓取第6页，目前总共抓取了 58 条微博\n",
      "抓取第7页，目前总共抓取了 68 条微博\n",
      "抓取第8页，目前总共抓取了 75 条微博\n",
      "抓取第9页，目前总共抓取了 84 条微博\n",
      "抓取第10页，目前总共抓取了 94 条微博\n"
     ]
    }
   ],
   "source": [
    "# 主函数（执行）\n",
    "if __name__ == '__main__':\n",
    "    fetch_data(\"768409523\", \"1076031768409523\") ## 给爬虫函数输入两个重要参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
